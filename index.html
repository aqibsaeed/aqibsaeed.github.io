<!doctype html>
<head>
  <title>Aaqib Saeed</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,600,300" rel="stylesheet" type="text/css">
  <meta name="theme-color" content="#1a4067" />
  <meta property="og:title" content="Aaqib Saeed - Researcher | Professor" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Assistant Professor and AI Researcher" />
  <meta property="og:url" content="https://aqibsaeed.github.io/" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <link rel="stylesheet" href="style.css">
</head>
<body onload="showPageCallback()"> 
<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/template.v1.js"></script>
<dt-article id="dtbody" style="display:none;">
<dt-byline class="l-page transparent"></dt-byline>
<div class="header">
  <img class="profile-image" src="assets/misc/aaqib_photo.jpg" alt="AS"/>
  <h1>Aaqib Saeed</h1>
   <h2>Researcher | Professor<br>
    Decentralized AI, Deep Learning, Self-Learning Sensing Systems
   </h2>
</div>
<dt-byline class="l-page mb-15" id="authors_section">
<div class="byline">
<p>I am an Assistant Professor (tenured) at <a class="link-colored" href="https://www.tue.nl/en" target="_blank">Eindhoven University of Technology (TU/e)</a> and my research is supported by an <a href="https://www.nwo.nl/en/researchprogrammes/national-growth-fund/ained/ained-fellowship-grants" target="_blank" class="link-colored">NGF AiNed Fellowship Grant</a>. I hold a Ph.D. <i>cum laude</i> from TU/e, where I conducted research in the <a href="https://www.tue.nl/en/our-university/departments/mathematics-and-computer-science">Department of Mathematics and Computer Science</a>, on self-supervised learning for sensory data (ECG, EEG, IMU, PPG and Audio). Previously, I completed MSc. <i>cum laude</i> in Computer Science with a specialization in Data Science and Smart Services at the <a href="https://www.utwente.nl/en/">University of Twente</a>. My research experience includes collaborations at <span class="link-colored">Google Brain</span> and at the <a href="https://www.cst.cam.ac.uk/">University of Cambridge's</a> <a href="https://mobile-systems.cl.cam.ac.uk/index.html">Mobile Systems Lab</a> as an industrial fellow. Before joining TU/e as faculty, I worked as a Research Scientist in AI at <a class="link-colored" href="https://www.research.philips.com/">Philips Research</a>, where I contributed to advancing AI for Personal Health and Sensing applications.</p>
</div>
</div>
<div class="byline">
  <div class="authors">
    <div class="author"> 
        <i class="fas fa-graduation-cap fa-fw" style="color:#003366b3"></i>  <a class="project-link" href="https://scholar.google.com/citations?user=O0nlHrkAAAAJ&hl=en" target="_blank">Scholar</a>
    </div>
    <div class="author"> 
        <i class="fab fa-github-alt fa-fw" style="color:#003366b3"></i>  <a class="project-link" href="https://github.com/aqibsaeed" target="_blank">GitHub</a>
    </div>
    <div class="author">
        <i class="fab fa-linkedin-in fa-fw" style="color:#003366b3"></i>  <a class="project-link" href="https://www.linkedin.com/in/aqibsaeed/" target="_blank">LinkedIn</a>
    </div>
    <div class="author">
    	<i class="far fa-envelope fa-fw" style="color:#003366b3"></i>  <a class="project-link" href="mailto:aqibsaeed@protonmail.com">Email</a>
    </div>
<!--     <div class="author">
    	<i class="far fa-handshake fa-fw" style="color:#003366b3"></i>  <a class="project-link" href="https://topmate.io/aaqib_saeed">Book a Meeting</a>
    </div>	   -->
  </div>
</div>
</dt-byline>
</dt-byline>
<dt-byline>
    <div class="byline">
        <h3>Research</h3>
        <p>My research interests span <span class="text-underline">AI for Human Senses</span>, <span class="text-underline">Self-Supervised Learning</span>, <span class="text-underline">Federated Learning</span>, <span class="text-underline">Audio Understanding</span> and their applications for improving <span class="text-underline">Personal Health</b>.</span>
    </div>
</dt-byline>
<dt-byline>
    <div class="byline">
        <h3>Selected Publications</h3>
	<p>View Full Publication List on <a class="project-link" href="https://scholar.google.com/citations?hl=en&user=O0nlHrkAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a></p>
    </div>
</dt-byline>
<table valign="top">
<!-- project block -->
<tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_ecgqa.svg" style="width:100%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Electrocardiogram–Language Model for Few-Shot Question Answering with Meta Learning</div>
      <dt-byline>
      <div class="byline"> 
	  <a class="project-link" href="https://arxiv.org/pdf/2410.14464" target="_blank">PDF</a>
      </div><br>Jialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo <u>Aaqib Saeed</u> @ <a class="project-link" href="https://chil.ahli.cc/" target="_blank">CHIL 2025</a><br><br>
      We propose a multimodal meta-learning approach that empowers large language models to answer complex clinical questions about ECGs using very limited labeled data. By integrating a pre-trained ECG encoder with a frozen LLM through a trainable fusion module, our method enables robust reasoning over ECG signals. Experiments show strong generalization and high accuracy on diverse ECG question-answering tasks, even in challenging few-shot scenarios. 
    </td>
</tr>
<!-- project block -->
<tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_care.svg" style="width:100%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning</div>
      <dt-byline>
      <div class="byline"> 
	  <a class="project-link" href="https://arxiv.org/pdf/2505.01199" target="_blank">PDF</a>
      </div><br>Tsai-Ning Wang, Lin-Lin Chen, Neil Zeghidour, <u>Aaqib Saeed</u> @ <a class="project-link" href="https://chil.ahli.cc/" target="_blank">CHIL 2025</a><br><br>
      Medical audio signals are vital for diagnosis but challenging to analyze due to limited labeled data and reliance on handcrafted features. We present CaReAQA, an audio-language model combining audio understanding with language-based reasoning for open-ended clinical questions. We also introduce CareSound, a benchmark dataset featuring annotated medical audio and paired QA samples to foster diagnostic research. CaReAQA achieves state-of-the-art accuracy in both open- and closed-ended diagnostic tasks, demonstrating strong generalization and potential for AI-powered clinical support.
    </td>
</tr>
<!-- project block -->
<tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_dbeta.svg" style="width:100%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners</div>
      <dt-byline>
      <div class="byline"> 
	  <a class="project-link" href="https://arxiv.org/pdf/2410.02131" target="_blank">PDF</a>
      </div><br>Manh Pham, <u>Aaqib Saeed</u>, Dong Ma @ <a class="project-link" href="https://icml.cc/" target="_blank">ICML 2025</a><br><br>
      We present D-BETA, a novel framework that integrates ECG signals and textual reports for improved cardiovascular diagnostics using cross-modal learning. It leverages a contrastive masked auto-encoder to robustly align physiological and textual data, overcoming modality gaps and data scarcity. By combining generative and discriminative strengths with enhanced loss functions and negative sampling, D-BETA achieves state-of-the-art results.
    </td>
</tr>
<!-- project block -->
<tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_fedns.svg" style="width:100%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Collaboratively Learning Federated Models from Noisy Decentralized Data</div>
      <dt-byline>
      <div class="byline"> 
	  <a class="project-link" href="https://haoyuan-l.github.io/fedns/" target="_blank">Project Page</a>
	  <a class="project-link" href="https://arxiv.org/pdf/2409.02189" target="_blank">PDF</a>
	  <a class="project-link" href="https://github.com/Decentralized-AI-Reserach-Lab/FedNS" target="_blank">Code</a>
	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/li2024collaboratively.bib" target="_blank">BibTex</a>
      </div><br>Haoyuan Li, Mathias Funk, Nezihe Merve Gürel, <u>Aaqib Saeed</u> @ <a class="project-link" href="https://www3.cs.stonybrook.edu/~ieeebigdata2024/SpecialSessions.html#SpecialSession8" target="_blank">IEEE BigData 2024</a><br><br>
      Short Paper @ <a class="project-link" href="https://dmlr.ai/cfp-icml24/" target="_blank">Data-centric Machine Learning Research - Workshop at ICML</a>
      <br><br>
      We present Federated Noise-Sifting (FedNS), a novel method to tackle noisy input data in federated learning (FL). By analyzing gradient norm distributions, FedNS identifies low-quality client data and adjusts the aggregation process accordingly. Our plug-and-play approach enhances existing FL strategies, significantly improving model performance in both IID and non-IID scenarios.
    </td>
</tr>
<!-- project block -->
<tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/deltamask_overview.png" style="width:90%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Federated Fine-Tuning of Foundation Models via Probabilistic Masking</div>
      <dt-byline>
      <div class="byline"> 
	  <a class="project-link" href="https://arxiv.org/pdf/2311.17299.pdf" target="_blank">PDF</a>
	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/tsouvalas2023masking.bib" target="_blank">BibTex</a>
      </div><br>Vasileios Tsouvalas, Yuki M Asano, <u>Aaqib Saeed</u> @ <a class="project-link" href="https://www3.cs.stonybrook.edu/~ieeebigdata2024/SpecialSessions.html#SpecialSession8" target="_blank">IEEE BigData 2024</a><br><br>
      Short Paper @ <a class="project-link" href="https://icml-fm-wild.github.io/" target="_blank">Foundation Models in the Wild - Workshop at ICML</a>
      <br><br>
      Foundation Models (FMs) have revolutionized machine learning with their adaptability and high performance across tasks; yet, their integration into Federated Learning (FL) is challenging due to substantial communication overhead from their extensive parameterization. <br><br>
      We present DeltaMask, a novel method that efficiently fine-tunes FMs in a FL setting at an ultra-low bitrate, well below 1 bpp. Our comprehensive evaluations across various datasets and architectures demonstrate DeltaMask efficiently achieves bitrates as low as 0.09 bpp, enhancing communication efficiency while maintaining FMs performance.
    </td>
</tr>
<!-- project block -->
 <tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_oda.svg" style="width:92%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">On Out-of-Distribution Detection for Audio with Deep Nearest Neighbors</div>
      <dt-byline>
      <div class="byline">  
	  <a class="project-link" href="https://zaharah.github.io/ood_audio/" target="_blank">Project Page</a>
	  <a class="project-link" href="https://arxiv.org/pdf/2210.15283.pdf" target="_blank">PDF</a> 
	  <a class="project-link" href="https://github.com/Zaharah/ood_audio" target="_blank">Code</a>
	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/bukhsh2023out.bib" target="_blank">BibTex</a>
      </div><br>Zaharah Bukhsh, Aaqib Saeed @ <a class="project-link" href="https://2023.ieeeicassp.org/">ICASSP 2023</a>
      <br><br>
      Out-of-distribution (OOD) detection is concerned with identifying data points that do not belong to the same distribution as the model's training data. For the safe deployment of predictive models in a real-world environment, it is critical to avoid making confident predictions on OOD inputs as it can lead to potentially dangerous consequences.<br><br>
      However, OOD detection remains largely an under-explored area in the audio (and speech) domain. This is despite the fact that audio is a central modality for many tasks, such as speaker diarization, spoken language identification, and sound event detection. To address this, we propose to leverage feature-space of the model with deep k-nearest neighbors to detect OOD samples. 
      <br>
    </td>
  </tr>	
  <!-- project block -->
  <tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img pt-20" src="assets/misc/overview_sg.svg" style="width:55%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image</div>
      <dt-byline>
      <div class="byline">
	  <a class="project-link" href="https://single-image-distill.github.io/" target="_blank">Project Page</a>     
	  <a class="project-link" href="https://arxiv.org/pdf/2112.00725.pdf" target="_blank">PDF</a> 
	  <a class="project-link" href="https://github.com/yukimasano/single-img-extrapolating" target="_blank">Code</a>
	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/asano2023extrapolating.bib" target="_blank">BibTex</a>
      </div><br>Yuki M. Asano*, Aaqib Saeed* @ <a class="project-link" href="https://iclr.cc/">ICLR 2023</a>
      <br><br> 
      We developed a framework for training neural networks from scratch using a <i>single image</i> by means of knowledge distillation from a supervised pretrained teacher.<br><br>
      We demonstrate that it is possible to extrapolate to semantic classes such as those of ImageNet using single datum as models' inputs. We leverage knowledge distillation for this and achieve performances of 74% on CIFAR-100, 69% on ImageNet, 75.2% on UCF-101, 51% on Kinetics-400 and by extending this method to audio, 84% on SpeechCommands.
    </td>
  </tr>	 
<!-- project block -->
<!--  <tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_aloe.svg" style="width:73%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Active Learning of Non-semantic Speech Tasks with Pretrained Models</div>
      <dt-byline>
      <div class="byline">  
	  <a class="project-link" href="https://arxiv.org/pdf/2211.00119.pdf" target="_blank">PDF</a> 
	  <a class="project-link" href="https://github.com/HarlinLee/ALOE" target="_blank">Code</a> 
	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/lee2023active.bib" target="_blank">BibTex</a>
      </div><br>Harlin Lee, <u>Aaqib Saeed</u>, Andrea L. Bertozzi @ <a class="project-link" href="https://2023.ieeeicassp.org/">ICASSP 2023</a>
      <br><br>
      Pretraining neural networks with massive unlabeled datasets has become popular as it equips the deep models with a better prior to solve downstream tasks. However, this approach generally assumes that for downstream tasks, we have access to annotated data of sufficient size. 
      In this work, we propose ALOE, a novel system for improving the data- and label-efficiency of non-semantic speech tasks with active learning. ALOE uses pre-trained models in conjunction with active learning to label data incrementally and learns classifiers for downstream tasks, thereby mitigating the need to acquire labeled data beforehand.
      <br>
    </td>
  </tr> -->
 <!-- project block -->
 <tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_cocoa.svg" style="width:90%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">COCOA: Cross Modality Contrastive Learning for Sensor Data</div>
      <dt-byline>
      <div class="byline">  
	  <a class="project-link" href="https://arxiv.org/pdf/2208.00467.pdf" target="_blank">PDF</a> 
	  <a class="project-link" href="https://dl.acm.org/doi/10.1145/3550316" target="_blank">Link (Official)</a> 
	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/deldari2022cocoa.bib" target="_blank">BibTex</a>
	      </div><br>Shohreh Deldari, Hao Xue, <u>Aaqib Saeed</u>, Daniel V. Smith, Flora D. Salim @ <a class="project-link" href="https://ubicomp.org/ubicomp2022/" target="_blank">ACM IMWUT 2022 - Ubicomp 2022</a>
      <br><br>
	We propose COCOA (Cross mOdality COntrastive leArning), a self-supervised model that employs a novel objective function to learn high-quality representations from multisensor data by computing the cross-correlation between different data modalities and minimizing the similarity between irrelevant instances.<br><br>
	We evaluate the effectiveness of COCOA across a range of datasets and against several self-supervised methods. COCOA is highly label-efficient than the other baselines including the fully supervised model while using only one-tenth of available labeled data.
    </td>
  </tr>
 <!-- project block -->
  <tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_fedstar.svg" style="width:80%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Federated Self-Training for Semi-Supervised Audio Recognition</div>
      <dt-byline>
      <div class="byline">
	  <a class="project-link" href="https://arxiv.org/pdf/2107.06877.pdf" target="_blank">PDF</a> 
	  <a class="project-link" href="https://dl.acm.org/doi/10.1145/3520128 " target="_blank">Link (Official)</a> 
	  <a class="project-link" href="https://github.com/FederatedML/FedSTAR " target="_blank">Code</a> 
          <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/tsouvalas2021federated.bib" target="_blank">BibTex</a>
        </div><br>Vasileios Tsouvalas, <u>Aaqib Saeed</u>, Tanir Ozcelebi @ <a class="project-link" href="https://dl.acm.org/journal/tecs" target="_blank">ACM TECS 2022</a><br><br>
	<a class="project-link" href="https://ieeexplore.ieee.org/document/9746356" target="_blank">Short Paper</a>@ <a class="project-link" href="https://2022.ieeeicassp.org/" target="_blank"> IEEE ICASSP 2022</a>
        <br><br>
	Federated Learning is a distributed machine learning paradigm dealing with decentralized and personal datasets. Since data reside on devices like smartphones and virtual assistants, labeling is entrusted to clients or labels are extracted in an automated way for learning models. However, in the case of audio data, acquiring semantic annotations can be prohibitively expensive and time-consuming. As a result, an abundance of audio samples remains unlabeled and unexploited. We propose FedSTAR, a semi-supervised learning approach for audio recognition. FedSTAR leverages unlabeled data via self-training to improve the generalization of audio models. <br><br>
	We show that with as little as 3% labeled data available, FedSTAR on average can improve the recognition rate by 13.28% compared to the fully supervised federated model. We further demonstrate that self-supervised pre-trained models can accelerate the training of on-device models, significantly improving convergence within fewer training rounds.
    </td>
  </tr>	 
  <!-- project block -->
  <tr>
   <td class="project-fig">
     <div class="figure">
      <img class="project-img" src="assets/misc/overview_headgesture.svg" style="width:60%;"/></div>
   </td>
    <td class="project-cell">
      <div class="project-title">Recognizing Head Gestures and Facial Expressions with Earbuds</div>
      <dt-byline>
      <div class="byline">
	  <a class="project-link" href="https://dl.acm.org/doi/pdf/10.1145/3462244.3479921" target="_blank">PDF</a> 
	  <a class="project-link" href="https://dl.acm.org/doi/abs/10.1145/3462244.3479921" target="_blank">Link (Official)</a> 
          <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/gashi2021hierarchical.bib" target="_blank">BibTex</a>
      </div><br>Shkurta Gashi, <u>Aaqib Saeed</u>, Alessandra Vicini, Elena Di Lascio, Silvia Santini @ <a class="project-link" href="https://icmi.acm.org/2021/" target="_blank">ACM ICMI 2021</a>
        <br><br>
	Head gestures and facial expressions -- like, e.g., nodding or smiling -- are important indicators of the quality of human interactions in physical meetings as well as in a computer-mediated environment. The automated systems able to recognize such behavioral cues can support and improve human interactions. <br><br>
        In this work, we consider inertial signals collected from unobtrusive, ear-mounted devices to recognize gestures and facial expressions typically performed during social interactions -- head shaking, nodding, smiling, talking, and yawning. We propose a hierarchical classification approach with transfer learning to improve the generalization and data efficiency of the predictive model using raw IMU data.
    </td>
  </tr>	 
  <!-- project block -->	
 <tr>
    <td class="project-fig"><div class="figure">
      <img class="project-img" src="assets/misc/overview_cola.svg" style="width:95%;"/>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Contrastive Learning of General-Purpose Audio Representations</div>
      <dt-byline>
      <div class="byline">
	  <a class="project-link" href="https://arxiv.org/pdf/2010.10915.pdf" target="_blank">PDF</a>
	  <a class="project-link" href="https://github.com/google-research/google-research/tree/master/cola" target="_blank">Code</a>    
	  <a class="project-link" href="https://ieeexplore.ieee.org/abstract/document/9413528" target="_blank">Link (Official)</a> 
    <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/saeed2020contrastive.bib" target="_blank">BibTex</a>
      </div><br>
      Aaqib Saeed, David Grangier, Neil Zeghidour @ <a class="project-link" href="https://2021.ieeeicassp.org/" target="_blank">IEEE ICASSP 2021</a>
      <br><br>
      We introduce COLA, a self-supervised pre-training approach for learning a general-purpose representation of audio. We build on top of recent advances in contrastive learning for computer vision and reinforcement learning 
      to design a lightweight, easy-to-implement self-supervised model of audio. <br><br>
      We pre-train embeddings on the large-scale Audioset database and transfer these representations to 9 diverse classification tasks, including speech, music, animal sounds, and acoustic scenes.
      We show that despite its simplicity, our method significantly outperforms previous self-supervised systems. 
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class="project-img" src="assets/misc/overview_charm.png" style="width:95%;"/>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Learning from Heterogeneous EEG Signals with Differentiable Channel Reordering</div>
      <dt-byline>
      <div class="byline">
	  <a class="project-link" href="https://web.archive.org/web/20221009212515/https://aqibsaeed.github.io/charm_paper" target="_blank">Project Page</a> 
	  <a class="project-link" href="https://arxiv.org/pdf/2010.13694.pdf" target="_blank">PDF</a>
	  <a class="project-link" href="https://ieeexplore.ieee.org/abstract/document/9413712" target="_blank">Link (Official)</a>
    <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/saeed2020learning.bib" target="_blank">BibTex</a>
      </div><br>
      Aaqib Saeed, David Grangier, Olivier Pietquin, Neil Zeghidour @ <a class="project-link" href="https://2021.ieeeicassp.org/" target="_blank">IEEE ICASSP 2021</a>
      <br><br>
	We propose CHARM, a method for training a single neural network across inconsistent input channels. 
	Our work is motivated by Electroencephalography (EEG), where data collection protocols from different headsets result in varying channel ordering and number,
	which limits the feasibility of transferring trained systems across datasets.<br><br>
	CHARM is differentiable and compatible with architectures (e.g. CNNs) that expect consistent channels. Across different input noising conditions we show its robustness. 
        We also successfully perform transfer learning between datasets collected with different EEG headsets. 
    </td>
  </tr>	
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class="project-img pt-20" src="assets/misc/overview_sal.svg" style="width:85%;"/>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Sense and Learn: Self-Supervision for Omnipresent Sensors</div>
      <dt-byline>
      <div class="byline">
	  <a class="project-link" href="https://web.archive.org/web/20221010020618/https://aqibsaeed.github.io/sense-and-learn" target="_blank">Project Page</a>
	  <a class="project-link" href="https://arxiv.org/pdf/2009.13233.pdf" target="_blank">PDF</a>
	  <a class="project-link" href="https://www.sciencedirect.com/science/article/pii/S2666827021000761" target="_blank">Link (Official)</a>
      	  <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/saeed2021sense.bib" target="_blank">BibTex</a>
      </div><br>
	      Aaqib Saeed, Victor Ungureanu, Beat Gfeller @ <a class="project-link" href="https://www.sciencedirect.com/journal/machine-learning-with-applications">Machine Learning with Applications</a>
      <br><br>
Looking for a way to utilize large-scale unlabeled sensory (time-series) data to improve generalization on downstream task with few-labeled datapoints? Try: <i>Sense and Learn</i>, a self-supervised learning framework.
<br><br>
We propose a suite of self-supervised pretext tasks for pre-training deep neural networks without semantic labels. We evaluate the quality of learned embedding with our framework on a wide variety of end-tasks with a linear classifier on top of a fixed encoder, effectiveness in the low-data regime, and transfer learning. Our approach opens up exciting possibilities for on-device continual learning without requiring supervision.
    </td>
  </tr>
  <!-- project block -->
  <tr>
    <td class="project-fig"><div class="figure">
      <img class="project-img" src="assets/misc/overview_sscl.svg" style="width:75%;"/>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence</div>
      <dt-byline>
      <div class="byline">
	      <a class="project-link" href="https://arxiv.org/pdf/2007.13018.pdf" target="_blank">PDF</a>
      <a class="project-link" href="https://ieeexplore.ieee.org/document/9141293" target="_blank">Link (Official)</a> 
      <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/9141293.bib" target="_blank">BibTex</a>
      </div><br>
      Aaqib Saeed, Flora D. Salim, Tanir Ozcelebi, Johan Lukkien @ <a class="project-link" href="https://ieee-iotj.org/" target="_blank">IEEE Internet of Things Journal 2020</a>
      <br><br>
We present a self-supervised method for learning multi-sensor representations in a federated learning setting from unlabeled and decentralized data. Our scalogram-signal correspondence learning (SSCL) technique utilize wavelet transform and a contrastive objective for training the deep network to determine if a given pair of a signal and its complementary view (i.e., a scalogram generated with wavelet transform) align with each other or not. 
<br><br>    
We extensively assess the quality of learned features with SSCL on diverse public datasets, which comprise signals like electroencephalography, blood volume pulse, accelerometer, and Wi-Fi channel state information. We conduct experiments to demonstrate our approach's effectiveness in both centralized and federated settings through linear classification. Mainly, SSCL significantly improves generalization in the low-data regime by reducing the volume of labeled data required through leveraging self-supervised learning.
    </td>
  </tr>
  <tr>
    <td class="project-fig"><div class="figure">
      <img class="project-img pt-20" src="assets/misc/overview_ssrl.svg" style="width:75%;"/>
    </div></td>
    <td class="project-cell">
      <div class="project-title">Multi-Task Self-Supervised Learning for Human Activity Detection</div>
      <dt-byline>
      <div class="byline">
      <a class="project-link" href="https://sites.google.com/view/self-supervised-ar/" target="_blank">Project Page</a>
      <a class="project-link" href="https://arxiv.org/pdf/1907.11879.pdf" target="_blank">PDF</a> 
      <a class="project-link" href="https://dl.acm.org/citation.cfm?id=3328932" target="_blank">Link (Official)</a> 
      <a class="project-link" href="https://github.com/aqibsaeed/aqibsaeed.github.io/blob/main/assets/bib/saeed2019multi.bib" target="_blank">BibTex</a>
      </div><br>
      Aaqib Saeed, Tanir Ozcelebi, Johan Lukkien @ <a class="project-link" href="https://imwut.acm.org/" target="_blank">IMWUT June 2019</a>- <a class="project-link" href="http://ubicomp.org/ubicomp2019" target="_blank">Ubicomp 2019</a> <br><br>
      <a class="project-link" href="https://drive.google.com/file/d/0B4M2lUVyJzS4WHVLWjdZeGVZLWVDb1puX3N2b19lc0xRQzMw/view">Workshop Paper</a>@&nbsp;<a class="project-link" href="https://sites.google.com/view/self-supervised-icml2019" target="_blank">Self-supervised Learning Workshop ICML 2019</a>
      <br><br>
      We've created a Transformation Prediction Network, a self-supervised neural network for representation learning from sensory data that does not require access to any form of semantic labels, e.g., activity classes in human context detection. We demonstrate that simple auxiliary tasks of recognizing signal transformations result in strong supervision for extracting high-level features that generalize well on the down-stream task; substantially improving performance under semi-supervised and transfer learning settings in the low-data regime. 
    </td>
  </tr>
</table>
</dt-article>
<dt-appendix id="dtappendix" style="display:none;">
<h2>Acknowledgments</h2>
<p>This article was prepared using the <a href="https://distill.pub">Distill</a> <a href="https://github.com/distillpub/template">template</a>.</p>
</dt-appendix></dt-appendix>
<style>
	 dt-article > h1:first-of-type {font-family: 'Open Sans'; font-weight: 900;}
	 dt-article h1 + h2 {font-family: 'Open Sans'; font-weight: 500;}
	 dt-article h2 {font-family: 'Open Sans'; font-weight: 500;}
	 dt-article h3 {font-family: 'Open Sans'; font-style:normal; font-weight: 500;}
	.byline p {font-size:16px; font-family: 'Open Sans'; font-weight: 300; color: black; line-height: 1.6;}
	.project-title {font-size: 20px; font-family: 'Open Sans'; font-weight: 500;}
	.project-link{font-weight: 400;}
	 dt-byline {font-size: 15px; line-height: 20px; margin-bottom: 30px; font-family: 'Open Sans'; font-weight: 300; color: black; line-height: 1.6;}
	 dt-byline p {margin-bottom: 0px;}
	.authors {margin-top: 15px;}
	.project-cell {padding-top: 20px;}
	.pt-20{padding-top: 20px;}
	dt-appendix{font-family: 'Open Sans'; font-weight: 300;}
	.mb-15{margin-bottom: 15px;}
	dt-article li{font-family: 'Open Sans'; margin: 0;}
	.box{padding: 10px; border: solid gray 1px; background: #d3d3d340; border-radius:2px;}

	.collapsible {
	    cursor: pointer;
	}

	.arrow {
	    display: inline-block;
	    width: 0.5rem;
	    height: 0.5rem;
	    margin-left: 0.5rem;
	    border: solid black;
	    border-width: 0 2px 2px 0;
	    transform: rotate(45deg);
	    transition: transform 0.2s ease;
	}

	.down {
	    transform: rotate(225deg);
	}

	.hidden {
	    display: none;
	}
</style> 
</body>
<script src="lib/blazy.js"></script>
<script>
	var bLazy = new Blazy({
	   success: function(){
	     updateCounter();
	   }
	});
	var imageLoaded = 0;
	function updateCounter() {
	   imageLoaded++;
	}
	var block_fn;
	function showPageCallback(){
	  block_fn = setTimeout(showPage, 300);
	}
	function showPage() {
	  document.getElementById("dtbody").style.display = "block";
	  document.getElementById("dtappendix").style.display = "block";
	}
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79826043-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-79826043-1');
</script>
